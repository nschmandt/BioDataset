{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V16553</th>\n",
       "      <th>V16554</th>\n",
       "      <th>V16555</th>\n",
       "      <th>V16556</th>\n",
       "      <th>V16557</th>\n",
       "      <th>V16558</th>\n",
       "      <th>V16559</th>\n",
       "      <th>V16560</th>\n",
       "      <th>V16561</th>\n",
       "      <th>V16562</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 16563 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   response  V1  V2  V3  V4  V5  V6  V7  V8  V9   ...    V16553  V16554  \\\n",
       "0         1   0   0   0   0   0   1   0   0   0   ...         1       0   \n",
       "1         1   0   0   0   0   0   1   0   0   0   ...         0       0   \n",
       "2         1   0   0   0   0   0   1   0   0   0   ...         0       0   \n",
       "3         1   0   0   0   0   0   1   0   0   0   ...         0       0   \n",
       "4         1   0   0   0   0   0   1   0   0   0   ...         0       0   \n",
       "\n",
       "   V16555  V16556  V16557  V16558  V16559  V16560  V16561  V16562  \n",
       "0       0       0       0       0       0       0       0       0  \n",
       "1       0       0       0       0       0       0       0       1  \n",
       "2       0       0       0       0       0       0       0       1  \n",
       "3       0       0       0       0       0       0       0       0  \n",
       "4       0       0       0       0       0       0       0       0  \n",
       "\n",
       "[5 rows x 16563 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('C:/Users/nts21/Documents/tempus_dataset')\n",
    "data=pd.read_table('takehome1.txt')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(530, 16563)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now that we know what kind of values we are working with, let's try to predict the responses column.\n",
    "#first, let's get rid of any columns that are all ones or zeros, as they are not helpful.\n",
    "col_to_use=list(data.columns)\n",
    "for i in data.columns:\n",
    "    if np.sum(data[i])==0 or np.sum(data[i])==530:\n",
    "        col_to_use.remove(i)\n",
    "col_to_use.remove('response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on training data with a logistic regression model:\n",
      "total f-score: 1.00\n",
      "total false positives: 0.00\n",
      "total false negatives: 0.00\n",
      "total true positives: 123.00\n"
     ]
    }
   ],
   "source": [
    "#OK, now we're ready to start trying to predict!\n",
    "#We'll start with simple logistic regression\n",
    "print('Predicting on training data with a logistic regression model:')\n",
    "model=LogisticRegression()\n",
    "fit=model.fit(data[col_to_use], data['response'])\n",
    "lr_prediction=fit.predict(data[col_to_use])\n",
    "temp=lr_prediction-data['response']\n",
    "false_pos=sum(temp==1)\n",
    "false_neg=sum(temp==-1)\n",
    "temp=lr_prediction+data['response']\n",
    "true_pos=sum(temp==2)\n",
    "precision=true_pos/(true_pos+false_pos)\n",
    "recall=true_pos/(true_pos+false_neg)\n",
    "f_score=2*precision*recall/(precision+recall)\n",
    "print('total f-score: %.2f' % (f_score))\n",
    "print('total false positives: %.2f' % (false_pos))\n",
    "print('total false negatives: %.2f' % (false_neg))\n",
    "print('total true positives: %.2f' % (true_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on 5x cross validated data with a logistic regression model:\n",
      "total f-score: 0.62\n",
      "total false positives: 2.00\n",
      "total false negatives: 13.00\n",
      "total true positives: 12.00\n",
      "total f-score: 0.67\n",
      "total false positives: 1.00\n",
      "total false negatives: 12.00\n",
      "total true positives: 13.00\n",
      "total f-score: 0.62\n",
      "total false positives: 4.00\n",
      "total false negatives: 12.00\n",
      "total true positives: 13.00\n",
      "total f-score: 0.63\n",
      "total false positives: 0.00\n",
      "total false negatives: 13.00\n",
      "total true positives: 11.00\n",
      "total f-score: 0.60\n",
      "total false positives: 4.00\n",
      "total false negatives: 12.00\n",
      "total true positives: 12.00\n"
     ]
    }
   ],
   "source": [
    "#A perfect fit to the training set. Let's see how it holds up if we break up the test and training sets. We'll cross \n",
    "#validate the data with five separate groups.\n",
    "print('Predicting on 5x cross validated data with a logistic regression model:')\n",
    "for i in range(5):\n",
    "    temp=list(range(5))\n",
    "    temp.remove(i)\n",
    "    val=data[i::5]\n",
    "    train=[data[temp[0]::5], data[temp[1]::5], data[temp[2]::5], data[temp[3]::5]]\n",
    "    train=pd.concat(train)\n",
    "    model=LogisticRegression()\n",
    "    fit=model.fit(train[col_to_use], train['response'])\n",
    "    lr_prediction=fit.predict(val[col_to_use])\n",
    "    temp=lr_prediction-val['response']\n",
    "    false_pos=sum(temp==1)\n",
    "    false_neg=sum(temp==-1)\n",
    "    temp=lr_prediction+val['response']\n",
    "    true_pos=sum(temp==2)\n",
    "    precision=true_pos/(true_pos+false_pos)\n",
    "    recall=true_pos/(true_pos+false_neg)\n",
    "    f_score=2*precision*recall/(precision+recall)\n",
    "    print('total f-score: %.2f' % (f_score))\n",
    "    print('total false positives: %.2f' % (false_pos))\n",
    "    print('total false negatives: %.2f' % (false_neg))\n",
    "    print('total true positives: %.2f' % (true_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on training data with a gradient boosting model:\n",
      "total f-score: 0.96\n",
      "total false positives: 0.00\n",
      "total false negatives: 9.00\n",
      "total true positives: 114.00\n"
     ]
    }
   ],
   "source": [
    "#The values for Logistic Regression look reasonable. Let's try Gradient Boosting now.\n",
    "print('Predicting on training data with a gradient boosting model:')\n",
    "model=GradientBoostingClassifier()\n",
    "fit=model.fit(data[col_to_use], data['response'])\n",
    "gb_prediction=fit.predict(data[col_to_use])\n",
    "temp=gb_prediction-data['response']\n",
    "false_pos=sum(temp==1)\n",
    "false_neg=sum(temp==-1)\n",
    "temp=gb_prediction+data['response']\n",
    "true_pos=sum(temp==2)\n",
    "precision=true_pos/(true_pos+false_pos)\n",
    "recall=true_pos/(true_pos+false_neg)\n",
    "f_score=2*precision*recall/(precision+recall)\n",
    "print('total f-score: %.2f' % (f_score))\n",
    "print('total false positives: %.2f' % (false_pos))\n",
    "print('total false negatives: %.2f' % (false_neg))\n",
    "print('total true positives: %.2f' % (true_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on 5x cross validated data with a gradient boosting model:\n",
      "total f-score: 0.68\n",
      "total false positives: 2.00\n",
      "total false negatives: 11.00\n",
      "total true positives: 14.00\n",
      "total f-score: 0.70\n",
      "total false positives: 1.00\n",
      "total false negatives: 11.00\n",
      "total true positives: 14.00\n",
      "total f-score: 0.73\n",
      "total false positives: 1.00\n",
      "total false negatives: 10.00\n",
      "total true positives: 15.00\n",
      "total f-score: 0.68\n",
      "total false positives: 1.00\n",
      "total false negatives: 11.00\n",
      "total true positives: 13.00\n",
      "total f-score: 0.62\n",
      "total false positives: 5.00\n",
      "total false negatives: 11.00\n",
      "total true positives: 13.00\n"
     ]
    }
   ],
   "source": [
    "#Pretty close, but not quite perfect on the training set. Let's try cross-validating it as well.\n",
    "print('Predicting on 5x cross validated data with a gradient boosting model:')\n",
    "for i in range(5):\n",
    "    temp=list(range(5))\n",
    "    temp.remove(i)\n",
    "    val=data[i::5]\n",
    "    train=[data[temp[0]::5], data[temp[1]::5], data[temp[2]::5], data[temp[3]::5]]\n",
    "    train=pd.concat(train)\n",
    "    model=GradientBoostingClassifier()\n",
    "    fit=model.fit(train[col_to_use], train['response'])\n",
    "    gb_prediction=fit.predict(val[col_to_use])\n",
    "    temp=gb_prediction-val['response']\n",
    "    false_pos=sum(temp==1)\n",
    "    false_neg=sum(temp==-1)\n",
    "    temp=gb_prediction+val['response']\n",
    "    true_pos=sum(temp==2)\n",
    "    precision=true_pos/(true_pos+false_pos)\n",
    "    recall=true_pos/(true_pos+false_neg)\n",
    "    f_score=2*precision*recall/(precision+recall)\n",
    "    print('total f-score: %.2f' % (f_score))\n",
    "    print('total false positives: %.2f' % (false_pos))\n",
    "    print('total false negatives: %.2f' % (false_neg))\n",
    "    print('total true positives: %.2f' % (true_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on training data with a random forest model:\n",
      "total f-score with 10 nodes: 0.96\n",
      "total false positives with 10 nodes: 1.00\n",
      "total false negatives with 10 nodes: 9.00\n",
      "total true positives with 10 nodes: 114.00\n",
      "total f-score with 25 nodes: 0.98\n",
      "total false positives with 25 nodes: 0.00\n",
      "total false negatives with 25 nodes: 4.00\n",
      "total true positives with 25 nodes: 119.00\n",
      "total f-score with 50 nodes: 1.00\n",
      "total false positives with 50 nodes: 0.00\n",
      "total false negatives with 50 nodes: 0.00\n",
      "total true positives with 50 nodes: 123.00\n",
      "total f-score with 75 nodes: 1.00\n",
      "total false positives with 75 nodes: 0.00\n",
      "total false negatives with 75 nodes: 0.00\n",
      "total true positives with 75 nodes: 123.00\n",
      "total f-score with 100 nodes: 1.00\n",
      "total false positives with 100 nodes: 0.00\n",
      "total false negatives with 100 nodes: 0.00\n",
      "total true positives with 100 nodes: 123.00\n",
      "total f-score with 200 nodes: 1.00\n",
      "total false positives with 200 nodes: 0.00\n",
      "total false negatives with 200 nodes: 0.00\n",
      "total true positives with 200 nodes: 123.00\n"
     ]
    }
   ],
   "source": [
    "#The gradient boosting did a pretty good job. Now let's try with a random forest. To prevent overfitting, let's see how many nodes\n",
    "#are necessary to fit the training set.\n",
    "print('Predicting on training data with a random forest model:')\n",
    "for j in [10, 25, 50, 75, 100, 200]:\n",
    "    model=RandomForestClassifier(j, n_jobs=-1)\n",
    "    fit=model.fit(data[col_to_use], data['response'])\n",
    "    rf_prediction=fit.predict(data[col_to_use])\n",
    "    temp=rf_prediction-data['response']\n",
    "    false_pos=sum(temp==1)\n",
    "    false_neg=sum(temp==-1)\n",
    "    temp=rf_prediction+data['response']\n",
    "    true_pos=sum(temp==2)\n",
    "    precision=true_pos/(true_pos+false_pos)\n",
    "    recall=true_pos/(true_pos+false_neg)\n",
    "    f_score=2*precision*recall/(precision+recall)\n",
    "    print('total f-score with %s nodes: %.2f' % (j, f_score))\n",
    "    print('total false positives with %s nodes: %.2f' % (j, false_pos))\n",
    "    print('total false negatives with %s nodes: %.2f' % (j, false_neg))\n",
    "    print('total true positives with %s nodes: %.2f' % (j, true_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on 5x cross validated data with a random forest model:\n",
      "with 10 nodes, total f-score of 0.47\n",
      "with 25 nodes, total f-score of 0.57\n",
      "with 50 nodes, total f-score of 0.61\n",
      "with 75 nodes, total f-score of 0.61\n",
      "with 100 nodes, total f-score of 0.61\n",
      "with 200 nodes, total f-score of 0.62\n"
     ]
    }
   ],
   "source": [
    "#Only about 50 nodes fits the data perfectly. Let's compare its performance on the cross-validated test data now\n",
    "print('Predicting on 5x cross validated data with a random forest model:')\n",
    "for j in [10, 25, 50, 75, 100, 200]:\n",
    "    f_score_vals=[]\n",
    "    for i in range(5):\n",
    "        temp=list(range(5))\n",
    "        temp.remove(i)\n",
    "        val=data[i::5]\n",
    "        train=[data[temp[0]::5], data[temp[1]::5], data[temp[2]::5], data[temp[3]::5]]\n",
    "        train=pd.concat(train)\n",
    "        model=RandomForestClassifier(j, n_jobs=-1)\n",
    "        fit=model.fit(train[col_to_use], train['response'])\n",
    "        rf_prediction=fit.predict(val[col_to_use])\n",
    "        temp=rf_prediction-val['response']\n",
    "        false_pos=sum(temp==1)\n",
    "        false_neg=sum(temp==-1)\n",
    "        temp=rf_prediction+val['response']\n",
    "        true_pos=sum(temp==2)\n",
    "        precision=true_pos/(true_pos+false_pos)\n",
    "        recall=true_pos/(true_pos+false_neg)\n",
    "        f_score=2*precision*recall/(precision+recall)\n",
    "        #print(f_score)\n",
    "        #print(false_pos)\n",
    "        #print(false_neg)\n",
    "        #print(true_pos)\n",
    "        f_score_vals.append(f_score)\n",
    "    print('with %s nodes, total f-score of %.2f' % (j, np.mean(f_score_vals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Our best results were probably with the gradient boosting model, though the models are all comparable.\n",
    "#Some future directions include optimizing the classification cutoff in the forest and logistic regression models, a major\n",
    "#limitation of the sklearn toolkit. Also, modeling should be attempted with a neural net predictor in keras, though my guess\n",
    "#would be performance would be comparable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
